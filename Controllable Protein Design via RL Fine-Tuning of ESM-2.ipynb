{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632677d7",
   "metadata": {},
   "source": [
    "## 1. Environment & Hardware Verification\n",
    "To perform efficient fine-tuning of **ESM-2** using **LoRA** and **RL**, a GPU is required to handle the high-dimensional tensor operations and gradient calculations.\n",
    "\n",
    "* **Tool:** `nvidia-smi` (NVIDIA System Management Interface)\n",
    "* **Purpose:** Confirms the presence of a CUDA-enabled device and monitors VRAM availability.\n",
    "* **Safety Check:** If no GPU is detected, the script terminates execution to prevent slow CPU processing or Out-Of-Memory (OOM) errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c75c69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GPU availability...\n",
      " GPU detected!\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Checking GPU availability...\")\n",
    "try:\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], text=True)\n",
    "    print(\" GPU detected!\")\n",
    "    print(gpu_info.split('\\n')[8])  \n",
    "except:\n",
    "    print(\" WARNING: No GPU detected! This notebook requires a GPU.\")\n",
    "    print(\"Go to Runtime > Change runtime type > Select 'T4 GPU'\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ee65e",
   "metadata": {},
   "source": [
    "## 2. Dependency Installation\n",
    "This project utilizes the Hugging Face ecosystem and Meta's FAIR-ESM tools to implement a parameter-efficient training pipeline.\n",
    "\n",
    "| Library | Primary Function |\n",
    "| :--- | :--- |\n",
    "| **transformers** | Provides the pre-trained ESM-2 model architecture and tokenizers. |\n",
    "| **peft** | Implements **LoRA**, enabling the tuning of a fraction (~1%) of model parameters. |\n",
    "| **accelerate** | Handles device placement and distributed training optimizations. |\n",
    "| **fair-esm** | Native Meta AI tools for working with Evolutionary Scale Modeling (ESM) weights. |\n",
    "| **wandb** | Used for experiment tracking and visualizing multi-objective reward trade-offs. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4841983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q transformers>=4.41.0 peft==0.7.1 accelerate==0.25.0\n",
    "!pip install -q datasets wandb\n",
    "!pip install -q fair-esm\n",
    "\n",
    "print(\" All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72668e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, get_cosine_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e96a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35404417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "GPU:Tesla T4\n",
      "Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:{torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6da3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanivarshithpc\u001b[0m (\u001b[33mmanivarshithpc-vignan-institute-of-technology-and-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"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\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2fd3d",
   "metadata": {},
   "source": [
    "## 3. Global Configuration & Experiment Setup\n",
    "This section defines the architectural and behavioral parameters for the fine-tuning process. We utilize a structured `Config` class to ensure all hyperparameters are tracked.\n",
    "\n",
    "### Key Components:\n",
    "* **Model Backbone:** `ESM-2 (650M parameters)` - A large-scale protein language model.\n",
    "* **LoRA Strategy:** Targets the **Self-Attention** modules (`q, k, v`) to adapt sequence generation with minimal parameter updates.\n",
    "* **RL Steering:** * **KL Coefficient:** Controls the trade-off between exploring new sequences and staying close to the biologically-valid base model.\n",
    "    * **Reward Weights:** A weighted sum approach to balance **Stability** (structural integrity), **Diversity** (novelty), and **Constraint Satisfaction**.\n",
    "* **Logging:** Integrated with **Weights & Biases (WandB)** for real-time monitoring of reward convergence and sequence entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c367bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Configuration loaded\n",
      "GPU Memory Available: 15.83 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20260119_115632-hiz5ojv1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/hiz5ojv1' target=\"_blank\">esm2-rl-experiment</a></strong> to <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/hiz5ojv1' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/hiz5ojv1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \n",
    "    model_name: str = \"facebook/esm2_t33_650M_UR50D\"  \n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = None\n",
    "    \n",
    "    # Generation configuration\n",
    "    max_seq_length: int = 64\n",
    "    min_seq_length: int = 32\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 50\n",
    "    top_p: float = 0.9\n",
    "    \n",
    "    # RL training configuration\n",
    "    num_epochs: int = 5\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    num_sequences_per_batch: int = 8\n",
    "    learning_rate: float = 5e-5\n",
    "    kl_coef: float = 0.1  # KL penalty coefficient\n",
    "    clip_range: float = 0.2  \n",
    "    \n",
    "    # Reward weights\n",
    "    stability_weight: float = 1.0\n",
    "    diversity_weight: float = 0.5\n",
    "    constraint_weight: float = 0.5\n",
    "    \n",
    "    # Optimizer\n",
    "    adam_epsilon: float = 1e-8\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 100\n",
    "    \n",
    "    \n",
    "    log_interval: int = 10\n",
    "    save_interval: int = 100\n",
    "    use_wandb: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.lora_target_modules is None:\n",
    "            self.lora_target_modules = [\"query\", \"key\", \"value\"]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n Configuration loaded\")\n",
    "print(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "if config.use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"protein-rl-design\",\n",
    "        config=vars(config),\n",
    "        name=\"esm2-rl-experiment\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b905b33",
   "metadata": {},
   "source": [
    "## 4. Model Initialization & LoRA Adaptation\n",
    "This section loads the ESM-2 weights and configures the **Parameter-Efficient Fine-Tuning (PEFT)** setup.\n",
    "\n",
    "### Dual-Model Architecture:\n",
    "1.  **Active Model (Actor):** The `base_model` integrated with **LoRA** adapters. This is the only part of the network that will undergo gradient updates.\n",
    "2.  **Reference Model:** A frozen copy of the original ESM-2. It is used to calculate the **KL Penalty**, ensuring the RL agent remains within the manifold of biologically plausible sequences.\n",
    "\n",
    "### Efficiency Optimizations:\n",
    "* **FP16 Precision:** Reduces memory consumption to fit the 650M parameter model on a single GPU.\n",
    "* **LoRA Injection:** Targets the Query, Key, and Value matrices, allowing the model to adapt its attention patterns toward desired structural traits with <1% trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd3d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 33\n",
      "\n",
      "Base model parameters: 651.04M\n",
      "trainable params: 2,027,520 || all params: 653,070,774 || trainable%: 0.3104594602483314\n",
      "\n",
      " Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "print(f\"\\nBase model parameters: {sum(p.numel() for p in base_model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# Configure LoRA \n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=config.lora_target_modules,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "ref_model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "ref_model = ref_model.to(device)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"\\n Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c9a156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM-2 Tokenizer Inspection\n",
      "================================================================================\n",
      "Vocabulary size: 33\n",
      "\n",
      "Special tokens:\n",
      "  CLS token: '<cls>' (ID: 0)\n",
      "  PAD token: '<pad>' (ID: 1)\n",
      "  EOS token: '<eos>' (ID: 2)\n",
      "  MASK token: '<mask>' (ID: 32)\n",
      "\n",
      "First 33 tokens:\n",
      "  ID  0: <cls>\n",
      "  ID  1: <pad>\n",
      "  ID  2: <eos>\n",
      "  ID  3: <unk>\n",
      "  ID  4: L\n",
      "  ID  5: A\n",
      "  ID  6: G\n",
      "  ID  7: V\n",
      "  ID  8: S\n",
      "  ID  9: E\n",
      "  ID 10: R\n",
      "  ID 11: T\n",
      "  ID 12: I\n",
      "  ID 13: D\n",
      "  ID 14: P\n",
      "  ID 15: K\n",
      "  ID 16: Q\n",
      "  ID 17: N\n",
      "  ID 18: F\n",
      "  ID 19: Y\n",
      "  ID 20: M\n",
      "  ID 21: H\n",
      "  ID 22: W\n",
      "  ID 23: C\n",
      "  ID 24: X\n",
      "  ID 25: B\n",
      "  ID 26: U\n",
      "  ID 27: Z\n",
      "  ID 28: O\n",
      "  ID 29: .\n",
      "  ID 30: -\n",
      "  ID 31: <null_1>\n",
      "  ID 32: <mask>\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"ESM-2 Tokenizer Inspection\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"\\nSpecial tokens:\")\n",
    "print(f\"  CLS token: '{tokenizer.cls_token}' (ID: {tokenizer.cls_token_id})\")\n",
    "print(f\"  PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"  MASK token: '{tokenizer.mask_token}' (ID: {tokenizer.mask_token_id})\")\n",
    "\n",
    "print(f\"\\nFirst 33 tokens:\")\n",
    "for i in range(len(tokenizer)):\n",
    "    token = tokenizer.convert_ids_to_tokens(i)\n",
    "    print(f\"  ID {i:2d}: {token}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5750d",
   "metadata": {},
   "source": [
    "## 5. Autoregressive Protein Generation\n",
    "Since **ESM-2** is natively a Masked Language Model, this module implements a custom generation wrapper to enable **de novo** protein design through iterative token prediction.\n",
    "\n",
    "### Key Sampling Techniques:\n",
    "* **Top-K Filtering:** Restricts sampling to the $K$ most likely next amino acids to prevent high-error transitions.\n",
    "* **Top-P (Nucleus) Sampling:** Dynamically chooses the smallest set of tokens whose cumulative probability exceeds threshold $P$, balancing diversity and structural validity.\n",
    "* **Log-Probability Tracking:** Crucial for the **Policy Gradient** updates in Reinforcement Learning, allowing the agent to correlate specific sequence choices with resulting rewards.\n",
    "\n",
    "### Generation Workflow:\n",
    "1.  **Initialize:** Start with the `[CLS]` token.\n",
    "2.  **Predict:** Generate logits for the next amino acid position.\n",
    "3.  **Filter & Sample:** Apply temperature, Top-K, and Top-P to select the next token.\n",
    "4.  **Iterate:** Append the token and repeat until `max_length` or `[EOS]` is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b1c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinGenerator:\n",
    "    \"\"\"Handles autoregressive protein sequence generation\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, config):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "        \n",
    "        self.cls_token_id = tokenizer.cls_token_id\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.mask_token_id = tokenizer.mask_token_id if hasattr(tokenizer, 'mask_token_id') else None\n",
    "        \n",
    "        self.valid_token_ids = list(range(4, 24))  \n",
    "        \n",
    "        self.valid_token_ids = torch.tensor(self.valid_token_ids, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ProteinGenerator initialized\")\n",
    "        print(f\"  Total vocabulary size: {len(tokenizer)}\")\n",
    "        print(f\"  Valid amino acid tokens: {len(self.valid_token_ids)}\")\n",
    "        print(f\"  Valid token IDs: {self.valid_token_ids.tolist()}\")\n",
    "        \n",
    "        \n",
    "        valid_tokens_str = [tokenizer.convert_ids_to_tokens(i) for i in self.valid_token_ids.tolist()]\n",
    "        print(f\"  Allowed amino acids: {', '.join(valid_tokens_str)}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "    def generate_sequences(\n",
    "        self, \n",
    "        num_sequences: int,\n",
    "        temperature: float = None,\n",
    "        max_length: int = None,\n",
    "        return_log_probs: bool = True\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], List[str]]:\n",
    "        \"\"\"Generate protein sequences autoregressively\"\"\"\n",
    "        temperature = temperature or self.config.temperature\n",
    "        max_length = max_length or self.config.max_seq_length\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "       \n",
    "        sequences = torch.full(\n",
    "            (num_sequences, 1), \n",
    "            self.cls_token_id, \n",
    "            dtype=torch.long, \n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        log_probs_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(max_length - 1):\n",
    "                outputs = self.model(sequences)\n",
    "                logits = outputs.logits[:, -1, :] \n",
    "                \n",
    "                \n",
    "                valid_logits = torch.full_like(logits, float('-inf'))\n",
    "                \n",
    "                \n",
    "                valid_logits[:, self.valid_token_ids] = logits[:, self.valid_token_ids]\n",
    "                \n",
    "                \n",
    "                valid_logits = valid_logits / temperature\n",
    "                \n",
    "                \n",
    "                valid_logits = self._top_k_top_p_filtering(\n",
    "                    valid_logits, \n",
    "                    self.config.top_k, \n",
    "                    self.config.top_p\n",
    "                )\n",
    "                \n",
    "                \n",
    "                if torch.isnan(valid_logits).any() or torch.isinf(valid_logits).all():\n",
    "                    print(f\"Warning: Invalid logits detected at step {step}\")\n",
    "                    valid_logits = torch.nan_to_num(valid_logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
    "                \n",
    "                probs = F.softmax(valid_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                \n",
    "                max_token_id = next_token.max().item()\n",
    "                if max_token_id >= len(self.tokenizer):\n",
    "                    raise ValueError(\n",
    "                        f\"Generated invalid token ID {max_token_id} \"\n",
    "                        f\"(vocab size: {len(self.tokenizer)})\"\n",
    "                    )\n",
    "                \n",
    "                if return_log_probs:\n",
    "                    log_prob = F.log_softmax(valid_logits, dim=-1)\n",
    "                    selected_log_probs = log_prob.gather(1, next_token)\n",
    "                    log_probs_list.append(selected_log_probs)\n",
    "                \n",
    "                sequences = torch.cat([sequences, next_token], dim=1)\n",
    "        \n",
    "        log_probs = torch.cat(log_probs_list, dim=1) if (return_log_probs and log_probs_list) else None\n",
    "        \n",
    "        \n",
    "        sequences_str = self.tokenizer.batch_decode(sequences, skip_special_tokens=True)\n",
    "        \n",
    "        return sequences, log_probs, sequences_str\n",
    "    \n",
    "    def _top_k_top_p_filtering(self, logits, top_k=0, top_p=1.0):\n",
    "        if top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8982c68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ProteinGenerator initialized\n",
      "  Total vocabulary size: 33\n",
      "  Valid amino acid tokens: 20\n",
      "  Valid token IDs: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "  Allowed amino acids: L, A, G, V, S, E, R, T, I, D, P, K, Q, N, F, Y, M, H, W, C\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Testing Fixed Generator (Standard Amino Acids Only)\n",
      "================================================================================\n",
      "✓ Successfully generated 3 sequences!\n",
      "\n",
      "Generated sequences:\n",
      "  1. W W W W W W W W W W W W W W W W W F W (length: 37)\n",
      "  2. E E E E E E E E E E E E E E E E E E E (length: 37)\n",
      "  3. S S S S S S S S S S S S S S S S S S S (length: 37)\n",
      "\n",
      "Verifying sequences contain only standard amino acids...\n",
      "  ✓ All sequences contain only standard amino acids!\n",
      "================================================================================\n",
      "\n",
      "✓ Generator is ready for training!\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "generator = ProteinGenerator(model, tokenizer, config)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing Fixed Generator (Standard Amino Acids Only)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    test_sequences, test_log_probs, test_str = generator.generate_sequences(\n",
    "        num_sequences=3, \n",
    "        max_length=20,\n",
    "        return_log_probs=True\n",
    "    )\n",
    "    print(f\" Successfully generated {len(test_str)} sequences!\")\n",
    "    print(f\"\\nGenerated sequences:\")\n",
    "    for i, seq in enumerate(test_str):\n",
    "        print(f\"  {i+1}. {seq} (length: {len(seq)})\")\n",
    "    \n",
    "\n",
    "    print(f\"\\nVerifying sequences contain only standard amino acids...\")\n",
    "    standard_aas = set('LAGVSERTIDPKQNFYMHWC')\n",
    "    all_valid = True\n",
    "    for seq in test_str:\n",
    "        seq_chars = set(seq.replace(' ', ''))\n",
    "        if not seq_chars.issubset(standard_aas):\n",
    "            print(f\"  Found invalid characters: {seq_chars - standard_aas}\")\n",
    "            all_valid = False\n",
    "    \n",
    "    if all_valid:\n",
    "        print(f\" All sequences contain only standard amino acids!\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "except Exception as e:\n",
    "    print(f\" Error during generation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(\"\\n Generator is ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20baf786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing Reward Functions\n",
      "================================================================================\n",
      "\n",
      "Sequence: MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQDNLSG...\n",
      "  Length: 75\n",
      "  Stability:   0.921\n",
      "  Diversity:   0.887\n",
      "  Constraint:  0.412\n",
      "  TOTAL:       1.570\n",
      "\n",
      "Sequence: GGGGGGGGGGGGGGGGGGGG...\n",
      "  Length: 20\n",
      "  Stability:  -0.991\n",
      "  Diversity:   0.462\n",
      "  Constraint: -0.083\n",
      "  TOTAL:      -0.802\n",
      "\n",
      "Sequence: AVILMFWYAVILMFWYAVIL...\n",
      "  Length: 20\n",
      "  Stability:   0.716\n",
      "  Diversity:   0.830\n",
      "  Constraint:  0.306\n",
      "  TOTAL:       1.284\n"
     ]
    }
   ],
   "source": [
    "class RewardCalculator:\n",
    "    \"\"\"Compute multiple reward signals for protein sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        # Amino acid properties\n",
    "        self.hydrophobic = set('AILMFWYV')\n",
    "        self.polar = set('STNQ')\n",
    "        self.charged = set('DEKR')\n",
    "        self.positive = set('KR')\n",
    "        self.negative = set('DE')\n",
    "        \n",
    "        \n",
    "        self.reference_sequences = []\n",
    "        \n",
    "    def compute_rewards(\n",
    "        self, \n",
    "        sequences: List[str]\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute combined and individual rewards\n",
    "        \n",
    "        Returns:\n",
    "            total_rewards: Combined reward (batch_size,)\n",
    "            reward_components: Dictionary of individual rewards\n",
    "        \"\"\"\n",
    "        batch_size = len(sequences)\n",
    "        \n",
    "        \n",
    "        stability_rewards = torch.zeros(batch_size)\n",
    "        diversity_rewards = torch.zeros(batch_size)\n",
    "        constraint_rewards = torch.zeros(batch_size)\n",
    "        \n",
    "        for i, seq in enumerate(sequences):\n",
    "            \n",
    "            stability_rewards[i] = self._stability_reward(seq)\n",
    "            diversity_rewards[i] = self._diversity_reward(seq)\n",
    "            constraint_rewards[i] = self._constraint_reward(seq)\n",
    "        \n",
    "        \n",
    "        stability_rewards = torch.tanh(stability_rewards)\n",
    "        diversity_rewards = torch.tanh(diversity_rewards)\n",
    "        constraint_rewards = torch.tanh(constraint_rewards)\n",
    "        \n",
    "        \n",
    "        total_rewards = (\n",
    "            self.config.stability_weight * stability_rewards +\n",
    "            self.config.diversity_weight * diversity_rewards +\n",
    "            self.config.constraint_weight * constraint_rewards\n",
    "        )\n",
    "        \n",
    "        reward_components = {\n",
    "            'stability': stability_rewards,\n",
    "            'diversity': diversity_rewards,\n",
    "            'constraint': constraint_rewards,\n",
    "            'total': total_rewards\n",
    "        }\n",
    "        \n",
    "        return total_rewards, reward_components\n",
    "    \n",
    "    def _stability_reward(self, seq: str) -> float:\n",
    "        \"\"\"\n",
    "        Stability proxy based on:\n",
    "        1. Hydrophobic core enrichment\n",
    "        2. Charge balance\n",
    "        3. Amino acid composition\n",
    "        \"\"\"\n",
    "        if len(seq) == 0:\n",
    "            return -1.0\n",
    "        \n",
    "        seq = seq.upper()\n",
    "        score = 0.0\n",
    "        \n",
    "       \n",
    "        hydrophobic_frac = sum(1 for aa in seq if aa in self.hydrophobic) / len(seq)\n",
    "        score += 1.0 - abs(hydrophobic_frac - 0.35) * 2  # Penalty if far from 35%\n",
    "        \n",
    "        \n",
    "        positive_count = sum(1 for aa in seq if aa in self.positive)\n",
    "        negative_count = sum(1 for aa in seq if aa in self.negative)\n",
    "        net_charge = abs(positive_count - negative_count) / len(seq)\n",
    "        score += 1.0 - net_charge * 2  \n",
    "        \n",
    "        \n",
    "        max_repeat = self._max_consecutive_repeat(seq)\n",
    "        score -= max_repeat * 0.2  \n",
    "        \n",
    "        \n",
    "        helix_formers = set('AELM')\n",
    "        sheet_formers = set('VIY')\n",
    "        ss_frac = sum(1 for aa in seq if aa in helix_formers or aa in sheet_formers) / len(seq)\n",
    "        score += ss_frac * 0.5\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _diversity_reward(self, seq: str) -> float:\n",
    "        \"\"\"\n",
    "        Diversity reward based on:\n",
    "        1. Amino acid diversity (Shannon entropy)\n",
    "        2. Distance from reference sequences\n",
    "        \"\"\"\n",
    "        if len(seq) == 0:\n",
    "            return -1.0\n",
    "        \n",
    "        seq = seq.upper()\n",
    "        score = 0.0\n",
    "        \n",
    "        \n",
    "        aa_counts = {}\n",
    "        for aa in seq:\n",
    "            aa_counts[aa] = aa_counts.get(aa, 0) + 1\n",
    "        \n",
    "        entropy = 0.0\n",
    "        for count in aa_counts.values():\n",
    "            prob = count / len(seq)\n",
    "            entropy -= prob * np.log2(prob + 1e-10)\n",
    "        \n",
    "        \n",
    "        normalized_entropy = entropy / 4.32\n",
    "        score += normalized_entropy\n",
    "        \n",
    "        \n",
    "        if self.reference_sequences:\n",
    "            min_distance = float('inf')\n",
    "            for ref_seq in self.reference_sequences:\n",
    "                distance = self._edit_distance(seq, ref_seq)\n",
    "                min_distance = min(min_distance, distance)\n",
    "            \n",
    "            \n",
    "            normalized_distance = min_distance / max(len(seq), 1)\n",
    "            score += normalized_distance * 0.5\n",
    "        else:\n",
    "            score += 0.5  \n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _constraint_reward(self, seq: str) -> float:\n",
    "        \"\"\"\n",
    "        Constraint satisfaction reward:\n",
    "        1. Length constraints\n",
    "        2. Forbidden motifs\n",
    "        3. Required motifs\n",
    "        \"\"\"\n",
    "        if len(seq) == 0:\n",
    "            return -1.0\n",
    "        \n",
    "        seq = seq.upper()\n",
    "        score = 0.0\n",
    "        \n",
    "        \n",
    "        target_length = (self.config.min_seq_length + self.config.max_seq_length) / 2\n",
    "        length_penalty = abs(len(seq) - target_length) / target_length\n",
    "        score += 1.0 - length_penalty\n",
    "        \n",
    "        # 2. Forbidden motifs (e.g., glycine-proline repeats)\n",
    "        forbidden_motifs = ['GGG', 'PPP', 'CCC']\n",
    "        for motif in forbidden_motifs:\n",
    "            if motif in seq:\n",
    "                score -= 0.5\n",
    "        \n",
    "        # 3. Desired motifs (e.g., catalytic triads, binding motifs)\n",
    "        # Example: reward presence of common functional motifs\n",
    "        desired_motifs = ['RGD', 'KRK']  \n",
    "        for motif in desired_motifs:\n",
    "            if motif in seq:\n",
    "                score += 0.3\n",
    "        \n",
    "        \n",
    "        rare_aas = set('WCM')\n",
    "        rare_frac = sum(1 for aa in seq if aa in rare_aas) / len(seq)\n",
    "        if rare_frac > 0.15:  # Penalize if >15% rare amino acids\n",
    "            score -= (rare_frac - 0.15) * 2\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _max_consecutive_repeat(self, seq: str) -> int:\n",
    "        \"\"\"Find longest consecutive repeat of same amino acid\"\"\"\n",
    "        if not seq:\n",
    "            return 0\n",
    "        max_len = 1\n",
    "        current_len = 1\n",
    "        for i in range(1, len(seq)):\n",
    "            if seq[i] == seq[i-1]:\n",
    "                current_len += 1\n",
    "                max_len = max(max_len, current_len)\n",
    "            else:\n",
    "                current_len = 1\n",
    "        return max_len\n",
    "    \n",
    "    def _edit_distance(self, s1: str, s2: str) -> int:\n",
    "        \n",
    "        if len(s1) < len(s2):\n",
    "            return self._edit_distance(s2, s1)\n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "        \n",
    "        previous_row = range(len(s2) + 1)\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = previous_row[j + 1] + 1\n",
    "                deletions = current_row[j] + 1\n",
    "                substitutions = previous_row[j] + (c1 != c2)\n",
    "                current_row.append(min(insertions, deletions, substitutions))\n",
    "            previous_row = current_row\n",
    "        \n",
    "        return previous_row[-1]\n",
    "    \n",
    "    def update_references(self, sequences: List[str]):\n",
    "        self.reference_sequences.extend(sequences)\n",
    "        \n",
    "        if len(self.reference_sequences) > 100:\n",
    "            self.reference_sequences = self.reference_sequences[-100:]\n",
    "\n",
    "reward_calculator = RewardCalculator(config)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing Reward Functions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_sequences = [\n",
    "    \"MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQDNLSGAEKAVQVKVKALPDAQFEVVHSLAK\",\n",
    "    \"GGGGGGGGGGGGGGGGGGGG\",  # Bad: poly-G\n",
    "    \"AVILMFWYAVILMFWYAVIL\"   # Good: hydrophobic rich\n",
    "]\n",
    "\n",
    "for seq in test_sequences:\n",
    "    total_reward, components = reward_calculator.compute_rewards([seq])\n",
    "    print(f\"\\nSequence: {seq[:50]}...\")\n",
    "    print(f\"  Length: {len(seq)}\")\n",
    "    print(f\"  Stability:  {components['stability'].item():6.3f}\")\n",
    "    print(f\"  Diversity:  {components['diversity'].item():6.3f}\")\n",
    "    print(f\"  Constraint: {components['constraint'].item():6.3f}\")\n",
    "    print(f\"  TOTAL:      {components['total'].item():6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "711698c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer:\n",
    "    \"\"\"PPO-style trainer for fine-tuning ESM-2 with LoRA\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        ref_model,\n",
    "        generator, \n",
    "        reward_calculator, \n",
    "        config\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.ref_model = ref_model\n",
    "        self.generator = generator\n",
    "        self.reward_calculator = reward_calculator\n",
    "        self.config = config\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            eps=config.adam_epsilon\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.scheduler = None\n",
    "        \n",
    "       \n",
    "        self.metrics = {\n",
    "            'rewards': [],\n",
    "            'kl_divergence': [],\n",
    "            'policy_loss': [],\n",
    "            'total_loss': [],\n",
    "            'avg_sequence_length': [],\n",
    "            'stability_rewards': [],\n",
    "            'diversity_rewards': [],\n",
    "            'constraint_rewards': []\n",
    "        }\n",
    "        \n",
    "    def train(self, num_epochs: int):\n",
    "        total_steps = num_epochs * (100 // self.config.batch_size)\n",
    "        self.scheduler = get_cosine_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Starting RL Training\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Epochs: {num_epochs}\")\n",
    "        print(f\"Batch size: {self.config.batch_size}\")\n",
    "        print(f\"Gradient accumulation steps: {self.config.gradient_accumulation_steps}\")\n",
    "        print(f\"Total training steps: {total_steps}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        global_step = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_metrics = {key: [] for key in self.metrics.keys()}\n",
    "            \n",
    "            num_batches = 100 // self.config.batch_size\n",
    "            pbar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for batch_idx in pbar:\n",
    "                self.model.eval()\n",
    "                sequences, _, sequences_str = self.generator.generate_sequences(\n",
    "                    num_sequences=self.config.num_sequences_per_batch,\n",
    "                    return_log_probs=False  \n",
    "                )\n",
    "                \n",
    "                \n",
    "                rewards, reward_components = self.reward_calculator.compute_rewards(sequences_str)\n",
    "                rewards = rewards.to(self.device)\n",
    "                \n",
    "                \n",
    "                self.model.train()\n",
    "                outputs = self.model(sequences)\n",
    "                train_log_probs = self._compute_log_probs(outputs.logits, sequences)\n",
    "                \n",
    "                # Compute reference log probs (for KL penalty) - no gradients\n",
    "                with torch.no_grad():\n",
    "                    ref_outputs = self.ref_model(sequences)\n",
    "                    ref_log_probs = self._compute_log_probs(ref_outputs.logits, sequences)\n",
    "                \n",
    "                \n",
    "                kl_div = (train_log_probs.detach() - ref_log_probs).mean()\n",
    "                \n",
    "                \n",
    "                policy_loss = -(train_log_probs * rewards.unsqueeze(1).detach()).mean()\n",
    "                kl_penalty = self.config.kl_coef * (train_log_probs - ref_log_probs.detach()).mean()\n",
    "                loss = policy_loss + kl_penalty\n",
    "                \n",
    "                \n",
    "                loss = loss / self.config.gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                \n",
    "                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), \n",
    "                        self.config.max_grad_norm\n",
    "                    )\n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "                \n",
    "                \n",
    "                epoch_metrics['rewards'].append(rewards.mean().item())\n",
    "                epoch_metrics['kl_divergence'].append(kl_div.item())\n",
    "                epoch_metrics['policy_loss'].append(policy_loss.item())\n",
    "                epoch_metrics['total_loss'].append(loss.item() * self.config.gradient_accumulation_steps)\n",
    "                epoch_metrics['avg_sequence_length'].append(np.mean([len(s) for s in sequences_str]))\n",
    "                epoch_metrics['stability_rewards'].append(reward_components['stability'].mean().item())\n",
    "                epoch_metrics['diversity_rewards'].append(reward_components['diversity'].mean().item())\n",
    "                epoch_metrics['constraint_rewards'].append(reward_components['constraint'].mean().item())\n",
    "                \n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'reward': f\"{rewards.mean().item():.3f}\",\n",
    "                    'loss': f\"{loss.item():.3f}\",\n",
    "                    'kl': f\"{kl_div.item():.3f}\"\n",
    "                })\n",
    "                \n",
    "                \n",
    "                if self.config.use_wandb and global_step % self.config.log_interval == 0:\n",
    "                    wandb.log({\n",
    "                        'reward': rewards.mean().item(),\n",
    "                        'policy_loss': policy_loss.item(),\n",
    "                        'kl_divergence': kl_div.item(),\n",
    "                        'total_loss': loss.item(),\n",
    "                        'learning_rate': self.scheduler.get_last_lr()[0],\n",
    "                        'epoch': epoch\n",
    "                    }, step=global_step)\n",
    "                \n",
    "                \n",
    "                self.reward_calculator.update_references(sequences_str)\n",
    "            \n",
    "            \n",
    "            for key in self.metrics.keys():\n",
    "                self.metrics[key].append(np.mean(epoch_metrics[key]))\n",
    "            \n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "            print(f\"  Avg Reward: {self.metrics['rewards'][-1]:.4f}\")\n",
    "            print(f\"  Avg Loss: {self.metrics['total_loss'][-1]:.4f}\")\n",
    "            print(f\"  Avg KL: {self.metrics['kl_divergence'][-1]:.4f}\")\n",
    "            print(f\"  Stability: {self.metrics['stability_rewards'][-1]:.4f}\")\n",
    "            print(f\"  Diversity: {self.metrics['diversity_rewards'][-1]:.4f}\")\n",
    "            print(f\"  Constraint: {self.metrics['constraint_rewards'][-1]:.4f}\")\n",
    "            \n",
    "            \n",
    "            print(f\"\\n  Sample sequences:\")\n",
    "            sample_seqs = sequences_str[:3]\n",
    "            for i, seq in enumerate(sample_seqs):\n",
    "                print(f\"    {i+1}. {seq}\")\n",
    "        \n",
    "        print(\"\\n Training completed!\")\n",
    "        return self.metrics\n",
    "    \n",
    "    def _compute_log_probs(self, logits: torch.Tensor, sequences: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute log probabilities for generated sequences\"\"\"\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = sequences[:, 1:].contiguous()\n",
    "        \n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        selected_log_probs = log_probs.gather(\n",
    "            dim=-1, \n",
    "            index=shift_labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "        \n",
    "        return selected_log_probs\n",
    "\n",
    "\n",
    "trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    generator=generator,\n",
    "    reward_calculator=reward_calculator,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274bc5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting RL Training\n",
      "================================================================================\n",
      "Epochs: 5\n",
      "Batch size: 4\n",
      "Gradient accumulation steps: 4\n",
      "Total training steps: 125\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6719e0fee0e04c8490dc9d4566333de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Avg Reward: 0.7145\n",
      "  Avg Loss: 0.8894\n",
      "  Avg KL: 0.0000\n",
      "  Stability: 0.7646\n",
      "  Diversity: 0.4445\n",
      "  Constraint: -0.5447\n",
      "\n",
      "  Sample sequences:\n",
      "    1. N N N N N N N N N N N N N N N N N N N N N K N N N N N N I N N N D I D K D I R I N F I I F F L K K N Y L Y S N I Y I N L N I N\n",
      "    2. K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K\n",
      "    3. R R R R R R R R R R R R K K K K K K K K K K Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85ba3ef095b424ba0cc4026e3ed169c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Avg Reward: 0.7307\n",
      "  Avg Loss: 0.9881\n",
      "  Avg KL: 0.0005\n",
      "  Stability: 0.7843\n",
      "  Diversity: 0.4401\n",
      "  Constraint: -0.5472\n",
      "\n",
      "  Sample sequences:\n",
      "    1. R R R R R R R R R R R G Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y\n",
      "    2. R R R R R R R R R R R R Y C T S S S S S S S S S S S S S S S S S S S S S G Y V C Y Y Y I F V C S L F C F F V F V Y Y L P F I D\n",
      "    3. R R R R R R R R R R K K K K K K K K K Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2153c0d529a476198acd4af1fa44a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Avg Reward: 0.7069\n",
      "  Avg Loss: 0.8393\n",
      "  Avg KL: 0.0014\n",
      "  Stability: 0.7718\n",
      "  Diversity: 0.4160\n",
      "  Constraint: -0.5457\n",
      "\n",
      "  Sample sequences:\n",
      "    1. R R R R R R R R R R R D N C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C C\n",
      "    2. S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S\n",
      "    3. G G G G G G G G G G G G G G G G G G G G G D Y V A G G V V W K G D L L E W E I R E V F D N Y T S I L I Y Y V A S F P P L D Y L\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b90a1944a5747889e05d1452de92fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "  Avg Reward: 0.7477\n",
      "  Avg Loss: 0.9814\n",
      "  Avg KL: 0.0029\n",
      "  Stability: 0.8025\n",
      "  Diversity: 0.4362\n",
      "  Constraint: -0.5457\n",
      "\n",
      "  Sample sequences:\n",
      "    1. N N N N N N N N N N N N N N N N N Y N T D Y D F Y Y L K S Y I Y K C D Y V Y N Y L V F K L I N S I Y N D T I F L Y L Y I L V I\n",
      "    2. E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E\n",
      "    3. R R R R R R R R R R K K R N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N I N Y T K I K K I S Y I K K\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89ca08e38c74912914eeca177f441b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = trainer.train(num_epochs=config.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde70b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

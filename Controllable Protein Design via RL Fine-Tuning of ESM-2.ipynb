{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c75c69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GPU availability...\n",
      "✓ GPU detected!\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Checking GPU availability...\")\n",
    "try:\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], text=True)\n",
    "    print(\"✓ GPU detected!\")\n",
    "    print(gpu_info.split('\\n')[8])  \n",
    "except:\n",
    "    print(\" WARNING: No GPU detected! This notebook requires a GPU.\")\n",
    "    print(\"Go to Runtime > Change runtime type > Select 'T4 GPU'\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4841983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q transformers>=4.41.0 peft==0.7.1 accelerate==0.25.0\n",
    "!pip install -q datasets wandb\n",
    "!pip install -q fair-esm\n",
    "\n",
    "print(\" All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72668e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, get_cosine_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e96a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35404417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "GPU:Tesla T4\n",
      "Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:{torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6da3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanivarshithpc\u001b[0m (\u001b[33mmanivarshithpc-vignan-institute-of-technology-and-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c367bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Configuration loaded\n",
      "GPU Memory Available: 15.83 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20260115_081933-nr1dvp3z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/nr1dvp3z' target=\"_blank\">esm2-rl-experiment</a></strong> to <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/nr1dvp3z' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/nr1dvp3z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \n",
    "    model_name: str = \"facebook/esm2_t33_650M_UR50D\"  \n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = None\n",
    "    \n",
    "    # Generation configuration\n",
    "    max_seq_length: int = 64\n",
    "    min_seq_length: int = 32\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 50\n",
    "    top_p: float = 0.9\n",
    "    \n",
    "    # RL training configuration\n",
    "    num_epochs: int = 5\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    num_sequences_per_batch: int = 8\n",
    "    learning_rate: float = 5e-5\n",
    "    kl_coef: float = 0.1  # KL penalty coefficient\n",
    "    clip_range: float = 0.2  \n",
    "    \n",
    "    # Reward weights\n",
    "    stability_weight: float = 1.0\n",
    "    diversity_weight: float = 0.5\n",
    "    constraint_weight: float = 0.5\n",
    "    \n",
    "    # Optimizer\n",
    "    adam_epsilon: float = 1e-8\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 100\n",
    "    \n",
    "    \n",
    "    log_interval: int = 10\n",
    "    save_interval: int = 100\n",
    "    use_wandb: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.lora_target_modules is None:\n",
    "            self.lora_target_modules = [\"query\", \"key\", \"value\"]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n Configuration loaded\")\n",
    "print(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "if config.use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"protein-rl-design\",\n",
    "        config=vars(config),\n",
    "        name=\"esm2-rl-experiment\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd3d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading ESM-2 Model with LoRA Adapters\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0841ecdff042939cab3299d61ce183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938b7ad694a142b48ffbb97264447d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c1625f148a485bae9096a2f0d9b083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7e00cf7d1a48aab034935e915da381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8a071b95b4447e86f7cb8e11f3eb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base model parameters: 652.36M\n",
      "trainable params: 2,027,520 || all params: 654,384,054 || trainable%: 0.3098364007506821\n",
      "\n",
      " Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading ESM-2 Model with LoRA Adapters\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "print(f\"\\nBase model parameters: {sum(p.numel() for p in base_model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=config.lora_target_modules,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM  \n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "ref_model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "ref_model = ref_model.to(device)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"\\n Models loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2f57e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3761564278.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProteinGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1c42c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

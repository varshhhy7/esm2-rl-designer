{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632677d7",
   "metadata": {},
   "source": [
    "## 1. Environment & Hardware Verification\n",
    "To perform efficient fine-tuning of **ESM-2** using **LoRA** and **RL**, a GPU is required to handle the high-dimensional tensor operations and gradient calculations.\n",
    "\n",
    "* **Tool:** `nvidia-smi` (NVIDIA System Management Interface)\n",
    "* **Purpose:** Confirms the presence of a CUDA-enabled device and monitors VRAM availability.\n",
    "* **Safety Check:** If no GPU is detected, the script terminates execution to prevent slow CPU processing or Out-Of-Memory (OOM) errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c75c69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GPU availability...\n",
      " GPU detected!\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Checking GPU availability...\")\n",
    "try:\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], text=True)\n",
    "    print(\" GPU detected!\")\n",
    "    print(gpu_info.split('\\n')[8])  \n",
    "except:\n",
    "    print(\" WARNING: No GPU detected! This notebook requires a GPU.\")\n",
    "    print(\"Go to Runtime > Change runtime type > Select 'T4 GPU'\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ee65e",
   "metadata": {},
   "source": [
    "## 2. Dependency Installation\n",
    "This project utilizes the Hugging Face ecosystem and Meta's FAIR-ESM tools to implement a parameter-efficient training pipeline.\n",
    "\n",
    "| Library | Primary Function |\n",
    "| :--- | :--- |\n",
    "| **transformers** | Provides the pre-trained ESM-2 model architecture and tokenizers. |\n",
    "| **peft** | Implements **LoRA**, enabling the tuning of a fraction (~1%) of model parameters. |\n",
    "| **accelerate** | Handles device placement and distributed training optimizations. |\n",
    "| **fair-esm** | Native Meta AI tools for working with Evolutionary Scale Modeling (ESM) weights. |\n",
    "| **wandb** | Used for experiment tracking and visualizing multi-objective reward trade-offs. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4841983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q transformers>=4.41.0 peft==0.7.1 accelerate==0.25.0\n",
    "!pip install -q datasets wandb\n",
    "!pip install -q fair-esm\n",
    "\n",
    "print(\" All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72668e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, get_cosine_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e96a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35404417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "GPU:Tesla T4\n",
      "Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:{torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6da3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanivarshithpc\u001b[0m (\u001b[33mmanivarshithpc-vignan-institute-of-technology-and-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"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\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2fd3d",
   "metadata": {},
   "source": [
    "## 3. Global Configuration & Experiment Setup\n",
    "This section defines the architectural and behavioral parameters for the fine-tuning process. We utilize a structured `Config` class to ensure all hyperparameters are tracked.\n",
    "\n",
    "### Key Components:\n",
    "* **Model Backbone:** `ESM-2 (650M parameters)` - A large-scale protein language model.\n",
    "* **LoRA Strategy:** Targets the **Self-Attention** modules (`q, k, v`) to adapt sequence generation with minimal parameter updates.\n",
    "* **RL Steering:** * **KL Coefficient:** Controls the trade-off between exploring new sequences and staying close to the biologically-valid base model.\n",
    "    * **Reward Weights:** A weighted sum approach to balance **Stability** (structural integrity), **Diversity** (novelty), and **Constraint Satisfaction**.\n",
    "* **Logging:** Integrated with **Weights & Biases (WandB)** for real-time monitoring of reward convergence and sequence entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c367bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Configuration loaded\n",
      "GPU Memory Available: 15.83 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">esm2-rl-experiment</strong> at: <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/8pam1c3z' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/8pam1c3z</a><br> View project at: <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260115_103806-8pam1c3z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20260115_104244-red5o31h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/red5o31h' target=\"_blank\">esm2-rl-experiment</a></strong> to <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/red5o31h' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/red5o31h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \n",
    "    model_name: str = \"facebook/esm2_t33_650M_UR50D\"  \n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = None\n",
    "    \n",
    "    # Generation configuration\n",
    "    max_seq_length: int = 64\n",
    "    min_seq_length: int = 32\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 50\n",
    "    top_p: float = 0.9\n",
    "    \n",
    "    # RL training configuration\n",
    "    num_epochs: int = 5\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    num_sequences_per_batch: int = 8\n",
    "    learning_rate: float = 5e-5\n",
    "    kl_coef: float = 0.1  # KL penalty coefficient\n",
    "    clip_range: float = 0.2  \n",
    "    \n",
    "    # Reward weights\n",
    "    stability_weight: float = 1.0\n",
    "    diversity_weight: float = 0.5\n",
    "    constraint_weight: float = 0.5\n",
    "    \n",
    "    # Optimizer\n",
    "    adam_epsilon: float = 1e-8\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 100\n",
    "    \n",
    "    \n",
    "    log_interval: int = 10\n",
    "    save_interval: int = 100\n",
    "    use_wandb: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.lora_target_modules is None:\n",
    "            self.lora_target_modules = [\"query\", \"key\", \"value\"]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n Configuration loaded\")\n",
    "print(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "if config.use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"protein-rl-design\",\n",
    "        config=vars(config),\n",
    "        name=\"esm2-rl-experiment\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 33\n",
      "\n",
      "Base model parameters: 651.04M\n",
      "trainable params: 2,027,520 || all params: 653,070,774 || trainable%: 0.3104594602483314\n",
      "\n",
      "âœ“ Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "print(f\"\\nBase model parameters: {sum(p.numel() for p in base_model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# Configure LoRA \n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=config.lora_target_modules,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION  \n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "ref_model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "ref_model = ref_model.to(device)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"\\n Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1c42c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632677d7",
   "metadata": {},
   "source": [
    "## 1. Environment & Hardware Verification\n",
    "To perform efficient fine-tuning of **ESM-2** using **LoRA** and **RL**, a GPU is required to handle the high-dimensional tensor operations and gradient calculations.\n",
    "\n",
    "* **Tool:** `nvidia-smi` (NVIDIA System Management Interface)\n",
    "* **Purpose:** Confirms the presence of a CUDA-enabled device and monitors VRAM availability.\n",
    "* **Safety Check:** If no GPU is detected, the script terminates execution to prevent slow CPU processing or Out-Of-Memory (OOM) errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c75c69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GPU availability...\n",
      " GPU detected!\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Checking GPU availability...\")\n",
    "try:\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], text=True)\n",
    "    print(\" GPU detected!\")\n",
    "    print(gpu_info.split('\\n')[8])  \n",
    "except:\n",
    "    print(\" WARNING: No GPU detected! This notebook requires a GPU.\")\n",
    "    print(\"Go to Runtime > Change runtime type > Select 'T4 GPU'\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ee65e",
   "metadata": {},
   "source": [
    "## 2. Dependency Installation\n",
    "This project utilizes the Hugging Face ecosystem and Meta's FAIR-ESM tools to implement a parameter-efficient training pipeline.\n",
    "\n",
    "| Library | Primary Function |\n",
    "| :--- | :--- |\n",
    "| **transformers** | Provides the pre-trained ESM-2 model architecture and tokenizers. |\n",
    "| **peft** | Implements **LoRA**, enabling the tuning of a fraction (~1%) of model parameters. |\n",
    "| **accelerate** | Handles device placement and distributed training optimizations. |\n",
    "| **fair-esm** | Native Meta AI tools for working with Evolutionary Scale Modeling (ESM) weights. |\n",
    "| **wandb** | Used for experiment tracking and visualizing multi-objective reward trade-offs. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4841983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q transformers>=4.41.0 peft==0.7.1 accelerate==0.25.0\n",
    "!pip install -q datasets wandb\n",
    "!pip install -q fair-esm\n",
    "\n",
    "print(\" All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72668e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, get_cosine_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e96a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35404417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "GPU:Tesla T4\n",
      "Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:{torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6da3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanivarshithpc\u001b[0m (\u001b[33mmanivarshithpc-vignan-institute-of-technology-and-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"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\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2fd3d",
   "metadata": {},
   "source": [
    "## 3. Global Configuration & Experiment Setup\n",
    "This section defines the architectural and behavioral parameters for the fine-tuning process. We utilize a structured `Config` class to ensure all hyperparameters are tracked.\n",
    "\n",
    "### Key Components:\n",
    "* **Model Backbone:** `ESM-2 (650M parameters)` - A large-scale protein language model.\n",
    "* **LoRA Strategy:** Targets the **Self-Attention** modules (`q, k, v`) to adapt sequence generation with minimal parameter updates.\n",
    "* **RL Steering:** * **KL Coefficient:** Controls the trade-off between exploring new sequences and staying close to the biologically-valid base model.\n",
    "    * **Reward Weights:** A weighted sum approach to balance **Stability** (structural integrity), **Diversity** (novelty), and **Constraint Satisfaction**.\n",
    "* **Logging:** Integrated with **Weights & Biases (WandB)** for real-time monitoring of reward convergence and sequence entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c367bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Configuration loaded\n",
      "GPU Memory Available: 15.83 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20260119_045634-ercguh5c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/ercguh5c' target=\"_blank\">esm2-rl-experiment</a></strong> to <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/ercguh5c' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/ercguh5c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \n",
    "    model_name: str = \"facebook/esm2_t33_650M_UR50D\"  \n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = None\n",
    "    \n",
    "    # Generation configuration\n",
    "    max_seq_length: int = 64\n",
    "    min_seq_length: int = 32\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 50\n",
    "    top_p: float = 0.9\n",
    "    \n",
    "    # RL training configuration\n",
    "    num_epochs: int = 5\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    num_sequences_per_batch: int = 8\n",
    "    learning_rate: float = 5e-5\n",
    "    kl_coef: float = 0.1  # KL penalty coefficient\n",
    "    clip_range: float = 0.2  \n",
    "    \n",
    "    # Reward weights\n",
    "    stability_weight: float = 1.0\n",
    "    diversity_weight: float = 0.5\n",
    "    constraint_weight: float = 0.5\n",
    "    \n",
    "    # Optimizer\n",
    "    adam_epsilon: float = 1e-8\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 100\n",
    "    \n",
    "    \n",
    "    log_interval: int = 10\n",
    "    save_interval: int = 100\n",
    "    use_wandb: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.lora_target_modules is None:\n",
    "            self.lora_target_modules = [\"query\", \"key\", \"value\"]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n Configuration loaded\")\n",
    "print(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "if config.use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"protein-rl-design\",\n",
    "        config=vars(config),\n",
    "        name=\"esm2-rl-experiment\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b905b33",
   "metadata": {},
   "source": [
    "## 4. Model Initialization & LoRA Adaptation\n",
    "This section loads the ESM-2 weights and configures the **Parameter-Efficient Fine-Tuning (PEFT)** setup.\n",
    "\n",
    "### Dual-Model Architecture:\n",
    "1.  **Active Model (Actor):** The `base_model` integrated with **LoRA** adapters. This is the only part of the network that will undergo gradient updates.\n",
    "2.  **Reference Model:** A frozen copy of the original ESM-2. It is used to calculate the **KL Penalty**, ensuring the RL agent remains within the manifold of biologically plausible sequences.\n",
    "\n",
    "### Efficiency Optimizations:\n",
    "* **FP16 Precision:** Reduces memory consumption to fit the 650M parameter model on a single GPU.\n",
    "* **LoRA Injection:** Targets the Query, Key, and Value matrices, allowing the model to adapt its attention patterns toward desired structural traits with <1% trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd3d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 33\n",
      "\n",
      "Base model parameters: 651.04M\n",
      "trainable params: 2,027,520 || all params: 653,070,774 || trainable%: 0.3104594602483314\n",
      "\n",
      " Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "print(f\"\\nBase model parameters: {sum(p.numel() for p in base_model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# Configure LoRA \n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=config.lora_target_modules,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION  \n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "ref_model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "ref_model = ref_model.to(device)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"\\n Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5750d",
   "metadata": {},
   "source": [
    "## 5. Autoregressive Protein Generation\n",
    "Since **ESM-2** is natively a Masked Language Model, this module implements a custom generation wrapper to enable **de novo** protein design through iterative token prediction.\n",
    "\n",
    "### Key Sampling Techniques:\n",
    "* **Top-K Filtering:** Restricts sampling to the $K$ most likely next amino acids to prevent high-error transitions.\n",
    "* **Top-P (Nucleus) Sampling:** Dynamically chooses the smallest set of tokens whose cumulative probability exceeds threshold $P$, balancing diversity and structural validity.\n",
    "* **Log-Probability Tracking:** Crucial for the **Policy Gradient** updates in Reinforcement Learning, allowing the agent to correlate specific sequence choices with resulting rewards.\n",
    "\n",
    "### Generation Workflow:\n",
    "1.  **Initialize:** Start with the `[CLS]` token.\n",
    "2.  **Predict:** Generate logits for the next amino acid position.\n",
    "3.  **Filter & Sample:** Apply temperature, Top-K, and Top-P to select the next token.\n",
    "4.  **Iterate:** Append the token and repeat until `max_length` or `[EOS]` is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b1c42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing Sequence Generation\n",
      "================================================================================\n",
      "\n",
      "Generated sequences:\n",
      "1. W W W W W W W W W W W W W W W W N N N N N N N N N N N N (length: 55)\n",
      "2. E E E E E E E E E E E E Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y (length: 79)\n",
      "3. S S S S S S S S S S S S S S S S S S S S S S N N N (length: 49)\n"
     ]
    }
   ],
   "source": [
    "class ProteinGenerator:\n",
    "    \"\"\"Handles autoregressive protein sequence generation\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, config):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "        \n",
    "        self.cls_token_id = tokenizer.cls_token_id\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        \n",
    "    def generate_sequences(\n",
    "        self, \n",
    "        num_sequences: int,\n",
    "        temperature: float = None,\n",
    "        max_length: int = None,\n",
    "        return_log_probs: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, List[str]]:\n",
    "        \"\"\"\n",
    "        Generate protein sequences autoregressively\n",
    "        \n",
    "        Returns:\n",
    "            sequences: Token IDs (batch_size, seq_len)\n",
    "            log_probs: Log probabilities (batch_size, seq_len)\n",
    "            sequences_str: Decoded sequences\n",
    "        \"\"\"\n",
    "        temperature = temperature or self.config.temperature\n",
    "        max_length = max_length or self.config.max_seq_length\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Start with CLS token\n",
    "        sequences = torch.full(\n",
    "            (num_sequences, 1), \n",
    "            self.cls_token_id, \n",
    "            dtype=torch.long, \n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        log_probs_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(max_length - 1):\n",
    "                \n",
    "                outputs = self.model(sequences)\n",
    "                logits = outputs.logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "                \n",
    "                \n",
    "                logits = logits / temperature\n",
    "                \n",
    "                \n",
    "                logits = self._top_k_top_p_filtering(\n",
    "                    logits, \n",
    "                    top_k=self.config.top_k, \n",
    "                    top_p=self.config.top_p\n",
    "                )\n",
    "                \n",
    "                \n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                \n",
    "                if return_log_probs:\n",
    "                    log_prob = F.log_softmax(logits, dim=-1)\n",
    "                    selected_log_probs = log_prob.gather(1, next_token)\n",
    "                    log_probs_list.append(selected_log_probs)\n",
    "                sequences = torch.cat([sequences, next_token], dim=1)\n",
    "                \n",
    "                if (next_token == self.eos_token_id).all():\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        if return_log_probs:\n",
    "            log_probs = torch.cat(log_probs_list, dim=1)\n",
    "        else:\n",
    "            log_probs = None\n",
    "        \n",
    "        sequences_str = self.tokenizer.batch_decode(sequences, skip_special_tokens=True)\n",
    "        return sequences, log_probs, sequences_str\n",
    "    \n",
    "    def _top_k_top_p_filtering(\n",
    "        self, \n",
    "        logits: torch.Tensor, \n",
    "        top_k: int = 0, \n",
    "        top_p: float = 1.0\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        if top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                1, sorted_indices, sorted_indices_to_remove\n",
    "            )\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        return logits\n",
    "\n",
    "generator = ProteinGenerator(model, tokenizer, config)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing Sequence Generation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_seqs, test_logprobs, test_strs = generator.generate_sequences(\n",
    "    num_sequences=3,\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated sequences:\")\n",
    "for i, seq in enumerate(test_strs):\n",
    "    print(f\"{i+1}. {seq} (length: {len(seq)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20baf786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4841983",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers>=4.41.0 peft==0.7.1 accelerate==0.25.0\n",
    "!pip install -q bitsandbytes==0.41.3 datasets wandb\n",
    "!pip install -q fair-esm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72668e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, get_cosine_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e96a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35404417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "GPU:Tesla T4\n",
      "Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:{torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6da3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanivarshithpc\u001b[0m (\u001b[33mmanivarshithpc-vignan-institute-of-technology-and-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c367bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20260115_072755-yk2c8p0c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/yk2c8p0c' target=\"_blank\">esm2-rl-experiment</a></strong> to <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/yk2c8p0c' target=\"_blank\">https://wandb.ai/manivarshithpc-vignan-institute-of-technology-and-science/protein-rl-design/runs/yk2c8p0c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = \"facebook/esm2-t33-650M-UR50D\" \n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = None\n",
    "    \n",
    "    # Generation configuration\n",
    "    max_seq_length: int = 64\n",
    "    min_seq_length: int = 32\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 50\n",
    "    top_p: float = 0.9\n",
    "    \n",
    "    # RL training configuration\n",
    "    num_epochs: int = 5\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    num_sequences_per_batch: int = 8\n",
    "    learning_rate: float = 5e-5\n",
    "    kl_coef: float = 0.1\n",
    "    clip_range: float = 0.2\n",
    "    \n",
    "    # Reward weights\n",
    "    stability_weight: float = 1.0\n",
    "    diversity_weight: float = 0.5\n",
    "    constraint_weight: float = 0.5\n",
    "    \n",
    "    # Optimizer\n",
    "    adam_epsilon: float = 1e-8\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 100\n",
    "    \n",
    "    # Logging\n",
    "    log_interval: int = 10\n",
    "    save_interval: int = 100\n",
    "    use_wandb: bool = True  # Enable W&B logging\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.lora_target_modules is None:\n",
    "            self.lora_target_modules = [\"query\", \"key\", \"value\"]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "\n",
    "if config.use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"protein-rl-design\",\n",
    "        config=vars(config),\n",
    "        name=\"esm2-rl-experiment\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c483583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
